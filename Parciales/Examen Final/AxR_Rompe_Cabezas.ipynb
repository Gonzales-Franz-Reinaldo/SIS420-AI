{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EXAMEN FINAL DE SIS420**\n",
    "\n",
    "## Nombre: Gonzales Suyo Franz Reinaldo\n",
    "## C.U. 35-5335\n",
    "## Carrera: Ing. Sistemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **APRENDIZAJE POR REFUERZO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción al Ejemplo de Rompecabezas\n",
    "\n",
    "El objetivo de este ejemplo es implementar un modelo de aprendizaje por refuerzo para resolver un rompecabezas de 4 filas por 5 columnas. Utilizaremos un enfoque basado en aprendizaje por refuerzo, similar al del juego de tres en raya, para enseñar a un agente a tomar decisiones óptimas que maximicen sus posibilidades de resolver el rompecabezas.\n",
    "\n",
    "\n",
    "### Descripción de la Implementación\n",
    "\n",
    "### Representación del Tablero del Rompecabezas:\n",
    "\n",
    "Se creará una clase PuzzleBoard para representar el tablero de 4 filas por 5 columnas.\n",
    "Esta clase manejará la lógica del juego, incluyendo la validación de movimientos, actualización del estado del tablero, verificación del estado del juego (si está ganado, perdido o empatado), y reinicio del tablero para nuevas partidas.\n",
    "Agente de Aprendizaje por Refuerzo:\n",
    "\n",
    "Se implementará una clase PuzzleAgent que representará al agente de aprendizaje por refuerzo.\n",
    "El agente utilizará una tabla de valores para estimar las probabilidades de éxito desde diferentes estados del tablero.\n",
    "El agente tomará decisiones basadas en una política de exploración-explotación, eligiendo a veces movimientos aleatorios (exploración) y otras veces movimientos que maximicen el valor esperado (explotación).\n",
    "La función de valor del agente se actualizará utilizando una fórmula basada en la diferencia temporal después de cada movimiento.\n",
    "\n",
    "#### Clase del Juego:\n",
    "\n",
    "Se definirá una clase PuzzleGame para gestionar las partidas entre dos agentes.\n",
    "Esta clase permitirá que los agentes jueguen múltiples partidas entre ellos, facilitando el entrenamiento de los agentes a través de la retroalimentación obtenida de los resultados de las partidas.\n",
    "Entrenamiento del Agente:\n",
    "\n",
    "Los agentes jugarán un gran número de partidas entre ellos para aprender las mejores estrategias.\n",
    "Durante el entrenamiento, los agentes ajustarán continuamente sus funciones de valor basadas en las recompensas recibidas y los estados visitados durante las partidas.\n",
    "Evaluación y Visualización de la Función de Valor:\n",
    "\n",
    "Después del entrenamiento, se evaluarán y ordenarán los estados del tablero por su valor estimado.\n",
    "Se utilizará una herramienta como Pandas para visualizar la función de valor de los agentes y analizar su desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1: Definir el Tablero del Rompecabezas\n",
    "Primero, definimos la clase PuzzleBoard para representar el tablero del rompecabezas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PuzzleBoard:\n",
    "    def __init__(self):\n",
    "        # Inicializa el estado del tablero como una matriz 4x5 de ceros\n",
    "        self.state = np.zeros((4, 5))\n",
    "    \n",
    "    def valid_moves(self):\n",
    "        # Retorna una lista de todas las posiciones vacías en el tablero\n",
    "        return [(i, j) for j in range(5) for i in range(4) if self.state[i, j] == 0]\n",
    "\n",
    "    def update(self, symbol, row, col):\n",
    "        # Actualiza el tablero con el símbolo del jugador en la posición (row, col)\n",
    "        if self.state[row, col] == 0:\n",
    "            self.state[row, col] = symbol\n",
    "        else:\n",
    "            raise ValueError(\"Movimiento ilegal!\")\n",
    "\n",
    "    def is_game_over(self):\n",
    "        # Verifica si el juego ha terminado\n",
    "        # Gana el jugador 1 si hay una fila o columna con suma 4\n",
    "        if (self.state.sum(axis=0) == 4).sum() >= 1 or (self.state.sum(axis=1) == 4).sum() >= 1:\n",
    "            return 1\n",
    "        # Gana el jugador 2 si hay una fila o columna con suma -4\n",
    "        if (self.state.sum(axis=0) == -4).sum() >= 1 or (self.state.sum(axis=1) == -4).sum() >= 1:\n",
    "            return -1\n",
    "        # Empate si no hay movimientos válidos\n",
    "        if len(self.valid_moves()) == 0:\n",
    "            return 0\n",
    "        # El juego sigue en curso\n",
    "        return None\n",
    "\n",
    "    def reset(self):\n",
    "        # Reinicia el tablero a su estado inicial\n",
    "        self.state = np.zeros((4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2: Definir el Agente de Aprendizaje por Refuerzo\n",
    "Ahora, definimos la clase PuzzleAgent que implementa el aprendizaje por refuerzo para el rompecabezas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PuzzleAgent:\n",
    "    def __init__(self, symbol, alpha=0.5, epsilon=0.1):\n",
    "        self.symbol = symbol        # Símbolo del agente (1 o -1)\n",
    "        self.alpha = alpha          # Tasa de aprendizaje\n",
    "        self.epsilon = epsilon      # Probabilidad de exploración\n",
    "        self.value_function = {}    # Función de valor (tabla de valores de estados)\n",
    "        self.positions = []         # Lista de posiciones visitadas en la partida actual\n",
    "\n",
    "    def reset(self):\n",
    "        # Reinicia la lista de posiciones visitadas\n",
    "        self.positions = []\n",
    "\n",
    "    def move(self, board, explore=True):\n",
    "        # Decide la siguiente jugada del agente\n",
    "        valid_moves = board.valid_moves()\n",
    "        if explore and np.random.uniform(0, 1) < self.epsilon:\n",
    "            # Exploración: elige un movimiento aleatorio\n",
    "            return valid_moves[np.random.choice(len(valid_moves))]\n",
    "        # Explotación: elige el movimiento con mayor valor estimado\n",
    "        max_value = -1000\n",
    "        for row, col in valid_moves:\n",
    "            next_board = board.state.copy()\n",
    "            next_board[row, col] = self.symbol\n",
    "            next_state = str(next_board.reshape(4 * 5))\n",
    "            value = self.value_function.get(next_state, 0.5)\n",
    "            if value >= max_value:\n",
    "                max_value = value\n",
    "                best_row, best_col = row, col\n",
    "        return best_row, best_col\n",
    "\n",
    "    def update(self, board):\n",
    "        # Almacena el estado del tablero después de cada movimiento\n",
    "        self.positions.append(str(board.state.reshape(4 * 5)))\n",
    "\n",
    "    def reward(self, reward):\n",
    "        # Actualiza la función de valor al final de la partida\n",
    "        for p in reversed(self.positions):\n",
    "            if self.value_function.get(p) is None:\n",
    "                self.value_function[p] = 0.5\n",
    "                \n",
    "            self.value_function[p] += self.alpha * (reward - self.value_function[p])\n",
    "            reward = self.value_function[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: Definir la Clase del Juego\n",
    "Definimos la clase PuzzleGame para gestionar las partidas entre dos agentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PuzzleGame:\n",
    "    def __init__(self, agent1, agent2):\n",
    "        self.board = PuzzleBoard()  # Crea el tablero del juego\n",
    "        self.agent1 = agent1        # Primer agente\n",
    "        self.agent2 = agent2        # Segundo agente\n",
    "\n",
    "    def play(self):\n",
    "        # Juega una partida completa entre los dos agentes\n",
    "        self.board.reset()          # Reinicia el tablero\n",
    "        self.agent1.reset()         # Reinicia el agente 1\n",
    "        self.agent2.reset()         # Reinicia el agente 2\n",
    "        turn = 1\n",
    "        while True:\n",
    "            if turn == 1:\n",
    "                row, col = self.agent1.move(self.board)\n",
    "                self.board.update(self.agent1.symbol, row, col)\n",
    "                self.agent1.update(self.board)\n",
    "                result = self.board.is_game_over()\n",
    "                if result is not None:\n",
    "                    self.agent1.reward(result)\n",
    "                    self.agent2.reward(-result)\n",
    "                    return result\n",
    "                turn = 2\n",
    "            else:\n",
    "                row, col = self.agent2.move(self.board)\n",
    "                self.board.update(self.agent2.symbol, row, col)\n",
    "                self.agent2.update(self.board)\n",
    "                result = self.board.is_game_over()\n",
    "                if result is not None:\n",
    "                    self.agent1.reward(-result)\n",
    "                    self.agent2.reward(result)\n",
    "                    return result\n",
    "                turn = 1\n",
    "\n",
    "    def selfplay(self, num_games):\n",
    "        # Hace que los agentes jueguen múltiples partidas entre ellos\n",
    "        results = []\n",
    "        for _ in range(num_games):\n",
    "            result = self.play()\n",
    "            results.append(result)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4: Entrenamiento de los Agentes\n",
    "Finalmente, entrenamos los agentes jugando múltiples partidas entre ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de los agentes\n",
    "agent1 = PuzzleAgent(symbol=1, epsilon=0.5)\n",
    "agent2 = PuzzleAgent(symbol=-1)\n",
    "\n",
    "# Creación del juego\n",
    "game = PuzzleGame(agent1, agent2)\n",
    "\n",
    "# Entrenamiento de los agentes\n",
    "results = game.selfplay(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   estado    valor\n",
      "0       [ 0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  ...  0.96875\n",
      "1       [ 1.  1. -1.  1. -1.  1.  0.  1.  1. -1.  1. -...  0.93750\n",
      "2       [ 1.  0. -1. -1.  1.  1. -1.  1. -1.  1.  1.  ...  0.93750\n",
      "3       [ 1.  0. -1. -1. -1.  1.  1.  1. -1.  1.  1. -...  0.93750\n",
      "4       [ 1.  0. -1.  1. -1.  1. -1.  1. -1.  1.  1. -...  0.93750\n",
      "...                                                   ...      ...\n",
      "317049  [ 0.  1. -1.  1. -1.  1.  1.  1. -1. -1.  1.  ...  0.06250\n",
      "317050  [ 0. -1.  1. -1. -1.  1.  1. -1.  1.  1.  1.  ...  0.06250\n",
      "317051  [ 1.  1.  1.  1. -1.  0. -1.  1. -1.  1. -1.  ...  0.06250\n",
      "317052  [ 0.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.  ...  0.06250\n",
      "317053  [ 1.  0. -1.  1. -1.  1.  1.  1.  1. -1.  1. -...  0.06250\n",
      "\n",
      "[317054 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Evaluación y visualización de la función de valor del agente 1\n",
    "funcion_de_valor = sorted(agent1.value_function.items(), key=lambda kv: kv[1], reverse=True)\n",
    "tabla = pd.DataFrame({'estado': [x[0] for x in funcion_de_valor], 'valor': [x[1] for x in funcion_de_valor]})\n",
    "\n",
    "print(tabla)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('agente.pickle', 'wb') as handle:\n",
    "    pickle.dump(agent1.value_function, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
