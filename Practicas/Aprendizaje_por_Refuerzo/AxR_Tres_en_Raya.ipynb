{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **APRENDIZAJE POR REFUERZO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "El aprendizaje por refuerzo explora una aproximación computacional al aprendizaje por interacción. De la misma manera que cuando aprendemos a conducir estamos atentos a cómo reacciona nuestro entorno a nuestras acciones y buscamos maneras de influenciarlo a través de nuestro comportamiento, el aprendizaje por refuerzo estudia cómo agentes computacionales pueden desarrollar comportamientos inteligentes a través de este tipo de interacción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apredizaje por Refuerzo\n",
    "\n",
    "El aprendizaje por refuerzo, de ahora en adelante **axr**, consiste en aprender qué hacer (cómo relacionar situaciones y acciones) con el objetivo de maximizar una recompensa numérica. En ningún momento especificamos qué acciones debe tomar un agente, sino que le dejamos descubrir cuales son las que le darán una mayor recompensa. En la mayoría de situaciones, una acción no solo afectará a la recompensa inmediata, si no que tendrá un efecto en todas las situaciones futuras. Estas dos propiedades, búsqueda por prueba y error y futuras recompensas, son las más importantes del axr.\n",
    "\n",
    "Para formalizar el problema del axr utilizamos ideas del campo de la teoría de sistemas dinámicos, en concreto el control óptimo de procesos de Markov incompletos. La idea básica consiste en aprender los aspectos más importantes sobre el problema real al que nuestro agente se enfrenta a través de la interacción con el entrono para conseguir su objetivo. El agente tiene que ser capaz de precibir su entorno y de llevar a cabo acciones que afecten a su estado. También necesita uno o varios objetivos relacionados con el estado del entorno. Un proceso de decisión de Markov incluye estos tres aspectos: percepción, acción y objetivo. Cualquier método que sea capaz de resolver este tipo de problemas se considera como un método de axr.\n",
    "\n",
    "El axr está considerado como un paradigma del *machine learning* diferente al aprendizaje supervisado y no supervisado. Se diferencia del aprendizaje supervisado en que no siempre será posible obtener ejemplos del comportamiento deseado para nuestro agente en cualquier tipo de situación en la que se pueda encontrar, por lo que deberá ser capaz de aprender de su propia experiencia. Por otro lado, se diferencia del aprendizaje no supervisado ya que éste no es capaz por si mismo de resolver el problema de maximización de la recompensa.\n",
    "\n",
    "El principal problema al que nos enfrentamos en el axr es el balance entre **exploración** y **explotación**. Para obtener la máxima recompensa, un agente podría escoger aquellas acciones que ya conoce que y que le direon buenos resultados. Sin embargo, el hecho de explorar nuevas acciones podría, eventualmente, dar mucho mejor resultado. Así pues, nuestro agente tiene que ser capaz de explotar su conocimiento para obtener recompensa pero también de explorar para descubrir mejores acciones. El problema es que ninguna de las dos aproximaciones puede llevarse a cabo de manera independiente para resolver un problema. Un agente debe probar muchas acciones y, poco a poco, favorecer aquellas que parezcan ser mejores. Este problema sigue sin estar resuelto.\n",
    "\n",
    "Otra propiedad importante que diferencia al axr de otro métodos es su capacidad de considerar todo el dominio del problema de un agente interactuando con su entrono, y no pequeñas partes o sub-tareas que puedan resolverse de manera independiente para llevar al objetivo final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos:\n",
    "\n",
    "Algunos ejemplos interesantes de aplicaciones del axr son:\n",
    "\n",
    "- Agentes que son capaces de jugar a juegos: ajedrez, go, atari, starcraft ...\n",
    "- Sistemas de control adaptativo en entornos industriales: refinerías, fábricas, cadenas de montaje, ...\n",
    "- Robótica\n",
    "- Conducción autónoma: el coche recibe información de su entorno a través de sus cámaras y sensores y ejecuta comandos para acelerar, frenar, girar el volante, ...\n",
    "\n",
    "Como vemos, todos los ejemplos comparten la existencia de un agente en constante interacción con su entorno para lograr un objetivo (a pesar de la posible incertidumbre). Tomar una decisión puede afectar a las acciones y oportunidades futuras, por lo que la elección de una accion requiere capacidad de planificación y predicción. Además, gracias a su interacción con el entorno, un agente puede adaptarse y aprender constantemente, ajustándose si es necesario para mejorar. De todas las formas de inteligencia computacional, el axr es el que más se asemeja a la forma en la que personas y animales actuamos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementos del AxR\n",
    "\n",
    "Además del agente y del entorno, existen cuatro subelementos esenciales en un sistema de axr:\n",
    "\n",
    "- **Política**: define el comportamiento de el agente en cada momento. Relaciona el estado que el agente percibe de su entorno con todas las posibles acciones que puede tomar. Puede ser tan simple como una función o una tabla, o tan complicada como un proceso de búsqueda. La política define completamente el comportamiento de un agente.\n",
    "- **Recompensa**: define el objetivo del problema, y es un un valor numérico que en cada momento el entorno envía al agente, el cual tiene el único objetivo de maximizarlo. Es el valor principal a tener en cuenta a la hora de alterar la política de un agente.\n",
    "- **Función de valor**: mientras que la recompensa indica la calidad de un estado de manera inmediata, la función de valor indica la calidad a largo plazo. El valor de un estado es la cantidad total de recompensa que un agente espera acumular en el futuro empezando en ese mismo estado. De esta manera, estados con una baja recompensa en relación a otros puede ser preferible si su valor es mayor (los estados futuros a los que nos permite llegar proporcionarán mayor recompensa). Así pues, favoreceremos acciones que impliquen mayor valor sobre recompensas inmediatas. El pricipal problema será estimar estos valores, ya que para ellos nuestro agente deberá explorar de manera repetida el entorno actualizando su información para poder llegar a ser capaz de tomar decisiones con fundamento.\n",
    "- **Modelo** del entorno: imita el comportamiento del entorno y sirve para planificar acciones considerando estados futuros que todavía no se han experimentado. El uso de un modelo del entorno nos permite dividir los métodos de axr en métodos sin modelos, en el que un agente aprende por prueba y error (lo opuesto a la planificación) y métodos con modelo, aunque en varias aplicaciones esta línea es un poco difusa ya que se combinan ambas opciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de la aplicación: Tres en raya\n",
    "\n",
    "\n",
    "Con el objetivo de ilustrar la idea general del axr vamos a considerar un ejemplo en detalle: el juego del tres en raya. En este juego, dos jugadores se turnan para dibujar una X o una O en un tablero con 3x3 posiciones. El primer jugador en conseguir dibujar tres figuras en una línea horizontal, vertical o diagonal, gana. Nuestro objetivo es conseguir un agente que sea capaz de ganar siempre a este juego.\n",
    "\n",
    "![](https://camo.githubusercontent.com/9b5f16d0451a8b6faf1ec45a6afbe22f55bbe1f9/68747470733a2f2f7468756d62732e6766796361742e636f6d2f506f697365644772697070696e67466f782d736d616c6c2e676966)\n",
    "\n",
    "\n",
    "\n",
    "Para resolver este juego con axr, en primer lugar definimos una tabla de números, uno por cada posible estado del juego. Cada numero respresentará la probabilidad de ganar el juego desde ese estado, el *valor* del estado. Así pues, la tabla sería la *función de valor*. Un estado $A$ es considerado mejor que un estado $B$ si el valor estimado de la probabilidad de ganar el juego desde $A$ es mayor que desde $B$. Si jugásemos con las Xs, todos los estados con tres X en raya tenría un valor de 1, ya que hemos ganado el juego. De la misma manera, cualquier estado con tres Os en raya tendría un valor de 0, hemos perdido. Para la inicialización de la tabla, podemos establecer el resto de valores en 0.5 (50% de posibilidades de ganar).\n",
    "\n",
    "Nuestro agente jugará muchas partidas contra un oponente (que puede ser otro agente). En cada turno evaluamos los estados que resultarían de cada posible movimiento (posiciones no ocupadas) y elegimos aquella con un mayor *valor*. Ocasionalmente, elegiremos una acción aleatoria con el objetivo de explorar nuevos movimientos.\n",
    "        \n",
    "Mientras el agente va jugando, tendremos que actualizar la función de valor. Para ello, después de cada movimiento, cambiaremos el valor del estado del que venimos para que se acerque al valor del estado actual.\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha [V(S_{t+1}) - V(S_t)]\n",
    "$$\n",
    "      \n",
    "donde $S_t$ denote el estado del que venimos, $S_{t+1}$ es el nuevo estado después del movimiento, $V(S_t)$ es el valor del estado $S_t$ y $\\alpha$ es el ratio de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Board():\n",
    "    def __init__(self):\n",
    "        # Inicializa el tablero del juego como una matriz de ceros 3x3\n",
    "        self.state = np.zeros((3,3))\n",
    "\n",
    "    # valid_moves: \n",
    "    def valid_moves(self):\n",
    "        # Devuelve una lista de tuplas que representan las posiciones válidas en el tablero donde se puede colocar un símbolo\n",
    "        return [(i, j) for j in range(3) for i in range(3) if self.state[i, j] == 0]\n",
    "\n",
    "    def update(self, symbol, row, col):\n",
    "        # Actualiza el estado del tablero con el símbolo (1 para X, -1 para O) en la fila y columna especificadas\n",
    "        if self.state[row, col] == 0:\n",
    "            self.state[row, col] = symbol\n",
    "        else:\n",
    "            raise ValueError(\"¡Movimiento ilegal!\")\n",
    "\n",
    "    def is_game_over(self):\n",
    "        # Comprueba si el juego ha terminado y devuelve el resultado (1 para X, -1 para O, 0 para empate, None si sigue el juego)\n",
    "        \n",
    "        # Comprueba filas y columnas\n",
    "        # Si la suma de una fila o columna es 3, X gana (1) y si es -3, O gana (-1)\n",
    "        if (self.state.sum(axis=0) == 3).sum() >= 1 or (self.state.sum(axis=1) == 3).sum() >= 1:\n",
    "            return 1\n",
    "        if (self.state.sum(axis=0) == -3).sum() >= 1 or (self.state.sum(axis=1) == -3).sum() >= 1:\n",
    "            return -1\n",
    "        \n",
    "        # Comprueba diagonales\n",
    "        # Si la suma de una diagonal es 3, X gana (1) y si es -3, O gana (-1)\n",
    "        diag_sums = [\n",
    "            sum([self.state[i, i] for i in range(3)]),\n",
    "            sum([self.state[i, 3 - i - 1] for i in range(3)]),\n",
    "        ]\n",
    "        \n",
    "        # Comprueba si hay un ganador\n",
    "        if diag_sums[0] == 3 or diag_sums[1] == 3:\n",
    "            return 1\n",
    "        if diag_sums[0] == -3 or diag_sums[1] == -3:\n",
    "            return -1\n",
    "        \n",
    "        # Comprueba si hay empate\n",
    "        if len(self.valid_moves()) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Continuar el juego\n",
    "        return None\n",
    "\n",
    "    def reset(self):\n",
    "        # Resetea el tablero del juego a su estado inicial (todos los ceros)\n",
    "        self.state = np.zeros((3,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Game():\n",
    "    def __init__(self, player1, player2):\n",
    "        # Asigna los símbolos de los jugadores (1 para player1 y -1 para player2)\n",
    "        player1.symbol = 1\n",
    "        player2.symbol = -1\n",
    "        # Almacena los jugadores y crea un tablero\n",
    "        self.players = [player1, player2]\n",
    "        self.board = Board()\n",
    "\n",
    "    # selfplay: función para que los jugadores jueguen entre sí\n",
    "    def selfplay(self, rounds=100):\n",
    "        # Inicializamos una lista para registrar las victorias de cada jugador\n",
    "        wins = [0, 0]\n",
    "        \n",
    "        # Itera sobre el número de rondas especificadas\n",
    "        for i in tqdm(range(1, rounds + 1)):\n",
    "            # Resetea el tablero y los jugadores para una nueva partida\n",
    "            self.board.reset()\n",
    "            \n",
    "            for player in self.players:\n",
    "                player.reset()\n",
    "                \n",
    "            game_over = False\n",
    "            \n",
    "            # Bucle principal del juego\n",
    "            while not game_over:\n",
    "                for player in self.players:\n",
    "                    # Cada jugador realiza un movimiento en el tablero\n",
    "                    action = player.move(self.board)\n",
    "                    self.board.update(player.symbol, action[0], action[1])\n",
    "                    \n",
    "                    # Actualiza el estado del jugador después del movimiento\n",
    "                    for player in self.players:\n",
    "                        player.update(self.board)\n",
    "                    \n",
    "                    # Verifica si el juego ha terminado\n",
    "                    if self.board.is_game_over() is not None:\n",
    "                        game_over = True\n",
    "                        break\n",
    "            \n",
    "            # Proporciona la recompensa al finalizar la partida\n",
    "            self.reward()\n",
    "            \n",
    "            # Registra las victorias de cada jugador\n",
    "            for ix, player in enumerate(self.players):\n",
    "                if self.board.is_game_over() == player.symbol:\n",
    "                    wins[ix] += 1\n",
    "        return wins\n",
    "\n",
    "\n",
    "    # reward: función para proporcionar recompensas a los jugadores al final de una partida\n",
    "    def reward(self):\n",
    "        # Determina el ganador del juego\n",
    "        winner = self.board.is_game_over()\n",
    "        \n",
    "        if winner == 0: # Si hay empate, ambos jugadores reciben una recompensa de 0.5\n",
    "            for player in self.players:\n",
    "                player.reward(0.5)\n",
    "        else: \n",
    "            # Si hay un ganador, el jugador que ganó recibe una recompensa de 1, mientras que el otro recibe 0\n",
    "            for player in self.players:\n",
    "                if winner == player.symbol:\n",
    "                    player.reward(1)\n",
    "                else:\n",
    "                    player.reward(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, alpha=0.5, prob_exp=0.55):\n",
    "        # Inicializa la función de valor como un diccionario vacío\n",
    "        self.value_function = {}  # tabla con pares estado -> valor\n",
    "        self.alpha = alpha  # tasa de aprendizaje\n",
    "        # Inicializa la lista de posiciones para almacenar los estados del juego\n",
    "        self.positions = []  # guardamos todas las posiciones de la partida\n",
    "        self.prob_exp = prob_exp  # probabilidad de exploración\n",
    "\n",
    "    def reset(self):\n",
    "        # Resetea la lista de posiciones al comienzo de cada partida\n",
    "        self.positions = []\n",
    "\n",
    "    # Funcion move: función para que el agente realice un movimiento en el tablero dado\n",
    "    def move(self, board, explore=True):\n",
    "        # Obtiene las posibles jugadas válidas en el tablero\n",
    "        valid_moves = board.valid_moves()\n",
    "        \n",
    "        # EXPLORACIÓN\n",
    "        if explore and np.random.uniform(0, 1) < self.prob_exp:\n",
    "            # Se elige una posición aleatoria si se está explorando\n",
    "            ix = np.random.choice(len(valid_moves))\n",
    "            return valid_moves[ix]\n",
    "        \n",
    "        # EXPLOTACIÓN\n",
    "        # Se elige la posición con el valor más alto en la función de valor\n",
    "        max_value = -1000\n",
    "        for row, col in valid_moves:\n",
    "            next_board = board.state.copy()\n",
    "            next_board[row, col] = self.symbol\n",
    "            next_state = str(next_board.reshape(3*3))\n",
    "            \n",
    "            value = 0 if self.value_function.get(next_state) is None else self.value_function.get(next_state)\n",
    "            \n",
    "            if value >= max_value:\n",
    "                max_value = value\n",
    "                best_row, best_col = row, col\n",
    "        return best_row, best_col\n",
    "\n",
    "    def update(self, board):\n",
    "        # Almacena el estado actual del tablero en la lista de posiciones\n",
    "        self.positions.append(str(board.state.reshape(3*3)))\n",
    "\n",
    "    # Obtiene la recompensa del estado actual\n",
    "    def reward(self, reward):\n",
    "        # Actualiza los valores de la función de valor al final de la partida\n",
    "        # Itera sobre los estados de la partida en orden inverso\n",
    "        for p in reversed(self.positions):\n",
    "            # Si el estado no está en la función de valor, lo inicializa a 0\n",
    "            if self.value_function.get(p) is None:\n",
    "                self.value_function[p] = 0\n",
    "                \n",
    "            # Actualiza el valor del estado utilizando la fórmula de actualización\n",
    "            self.value_function[p] += self.alpha * (reward - self.value_function[p])\n",
    "            # Actualiza la recompensa para el próximo estado\n",
    "            reward = self.value_function[p]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [33:38<00:00, 198.19it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[233112, 107487]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se crea una instancia del agente 1 con una probabilidad de exploración del 50%\n",
    "Agent1 = Agent(prob_exp=0.55)\n",
    "# Se crea una instancia del agente 2 con la configuración predeterminada (probabilidad de exploración 0.5)\n",
    "Agent2 = Agent()\n",
    "\n",
    "# Se crea una instancia del juego con los dos agentes creados anteriormente\n",
    "game = Game(Agent1, Agent2)\n",
    "\n",
    "# Los agentes juegan entre sí un número especificado de rondas (400000 en este caso)\n",
    "game.selfplay(400000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estado</th>\n",
       "      <th>Valor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1.  0. -1.  0. -1.  1.  1.  1.  1.]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ 0.  0.  1.  0.  1. -1.  1.  0. -1.]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ 1. -1. -1.  1.  1. -1.  1. -1.  1.]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ 0. -1. -1.  1.  1. -1.  1. -1.  1.]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ 0. -1.  1. -1. -1.  1.  1.  0.  1.]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5472</th>\n",
       "      <td>[ 0. -1.  1.  0. -1.  1.  1. -1.  0.]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5473</th>\n",
       "      <td>[-1. -1.  0.  1. -1.  1.  1.  0.  0.]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5474</th>\n",
       "      <td>[ 0.  1. -1. -1.  0.  1. -1.  1.  1.]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>[ 1. -1.  1.  1.  0.  1.  0. -1. -1.]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5476</th>\n",
       "      <td>[ 1.  1. -1. -1.  0. -1.  1.  1.  0.]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5477 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Estado  Valor\n",
       "0     [-1.  0. -1.  0. -1.  1.  1.  1.  1.]    1.0\n",
       "1     [ 0.  0.  1.  0.  1. -1.  1.  0. -1.]    1.0\n",
       "2     [ 1. -1. -1.  1.  1. -1.  1. -1.  1.]    1.0\n",
       "3     [ 0. -1. -1.  1.  1. -1.  1. -1.  1.]    1.0\n",
       "4     [ 0. -1.  1. -1. -1.  1.  1.  0.  1.]    1.0\n",
       "...                                     ...    ...\n",
       "5472  [ 0. -1.  1.  0. -1.  1.  1. -1.  0.]    0.0\n",
       "5473  [-1. -1.  0.  1. -1.  1.  1.  0.  0.]    0.0\n",
       "5474  [ 0.  1. -1. -1.  0.  1. -1.  1.  1.]    0.0\n",
       "5475  [ 1. -1.  1.  1.  0.  1.  0. -1. -1.]    0.0\n",
       "5476  [ 1.  1. -1. -1.  0. -1.  1.  1.  0.]    0.0\n",
       "\n",
       "[5477 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ordenamos la función de valor del Agente1 en orden descendente según los valores\n",
    "funcion_de_valor = sorted(Agent1.value_function.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "# Creamos un DataFrame de pandas a partir de la función de valor ordenada\n",
    "# con dos columnas: 'Estado' que contiene los estados del juego y 'Valor' que contiene los valores asociados\n",
    "tabla = pd.DataFrame({'Estado': [x[0] for x in funcion_de_valor], 'Valor': [x[1] for x in funcion_de_valor]})\n",
    "\n",
    "# Mostramos la tabla\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('agente.pickle', 'wb') as handle:\n",
    "    pickle.dump(Agent1.value_function, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo sirve para ilustrar algunas de las propiedades clave del axr. En primer lugar, aprender a través de la interacción con el entorno (en este caso el otro agente). En segundo lugar, tenemos un objetivo claro y el comportamiento correcto del agente requiere de planificación y predicción que tenga en cuenta los efectos futuros de sus acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "El aprendizaje por refuerzo es una aproximación computacional a la comprensión y automatización del aprendizaje por objetivos y toma de decisiones. En esta aproximación, una agente aprende a través de la interacción directa con su entorno sin necesidad de supervisión explícita. Utiliza procesos de decisión de Markov para definir la interacción entre el agente y su entorno en términos de estados, acciones y recompensas. Los conceptos de valor y función de valor son la clave de muchos métodos de axr ya que representan una manera eficiente de búsqueda en el espacio de políticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
