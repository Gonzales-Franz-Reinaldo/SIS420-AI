{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado.\n",
      "[[[[[ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]]\n",
      "\n",
      "   [[ 9.11256501 18.58127565]\n",
      "    [ 4.43169791 16.44690834]\n",
      "    [ 1.37895739 16.02329728]\n",
      "    [ 1.1490466  14.66299231]\n",
      "    [ 0.57992171 10.55516611]\n",
      "    [ 0.1        10.76887492]\n",
      "    [ 0.64602759 19.1718508 ]\n",
      "    [13.32372184  0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.1         0.        ]\n",
      "    [ 0.          1.58154876]\n",
      "    [ 0.          0.        ]]\n",
      "\n",
      "   [[84.27775877 67.0151474 ]\n",
      "    [99.99993067 77.41911966]\n",
      "    [93.1725438  76.74277701]\n",
      "    [99.98674383 99.62278216]\n",
      "    [99.9864701  90.08735566]\n",
      "    [99.98635329 93.82145893]\n",
      "    [99.96716711 99.98649181]\n",
      "    [98.55598324 99.98628202]\n",
      "    [99.98210657 99.98362315]\n",
      "    [93.92334105 99.40173158]\n",
      "    [86.31181539 99.97700312]\n",
      "    [39.52984452 96.10739341]]\n",
      "\n",
      "   [[94.6840806  41.1524035 ]\n",
      "    [99.94948863 93.41888816]\n",
      "    [98.62072517 93.61166873]\n",
      "    [99.97691677 99.82203484]\n",
      "    [99.87554071 99.25250278]\n",
      "    [99.79101042 99.97541709]\n",
      "    [99.84815368 99.88789458]\n",
      "    [91.17482093 98.46070711]\n",
      "    [99.14787506 99.55676514]\n",
      "    [92.24141101 80.78946674]\n",
      "    [78.54050979 93.46489604]\n",
      "    [76.80192788 92.92220177]]\n",
      "\n",
      "   [[ 0.          0.        ]\n",
      "    [ 1.44536904  0.        ]\n",
      "    [ 2.48264758  0.        ]\n",
      "    [80.83820532  0.1       ]\n",
      "    [78.02306943 21.39233202]\n",
      "    [ 9.49349674 71.27519649]\n",
      "    [ 4.44747787 58.26743621]\n",
      "    [17.18028373  6.56096857]\n",
      "    [ 2.3463356  18.88765507]\n",
      "    [19.42814258  2.25322088]\n",
      "    [13.70444766  0.91071906]\n",
      "    [20.32020694 11.28912382]]\n",
      "\n",
      "   [[ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]\n",
      "    [ 0.          0.        ]]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Crear el entorno\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Discretizar el espacio de observación\n",
    "num_buckets = (1, 1, 6, 12)  # Número de buckets para cada dimensión\n",
    "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "\n",
    "# Ajustar los límites de observación\n",
    "state_bounds[1] = [-0.5, 0.5]\n",
    "state_bounds[3] = [-np.radians(50), np.radians(50)]\n",
    "\n",
    "# Parámetros de Q-Learning\n",
    "alpha = 0.1  # Tasa de aprendizaje\n",
    "gamma = 0.99  # Factor de descuento\n",
    "epsilon = 1.0  # Tasa de exploración\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "num_episodes = 1000\n",
    "\n",
    "# Inicializar la tabla Q\n",
    "q_table = np.zeros(num_buckets + (env.action_space.n,))\n",
    "\n",
    "def discretize_state(state):\n",
    "    ratios = [(state[i] - state_bounds[i][0]) / (state_bounds[i][1] - state_bounds[i][0]) for i in range(len(state))]\n",
    "    new_state = [int(round((num_buckets[i] - 1) * ratios[i])) for i in range(len(state))]\n",
    "    new_state = [min(num_buckets[i] - 1, max(0, new_state[i])) for i in range(len(state))]\n",
    "    return tuple(new_state)\n",
    "\n",
    "def choose_action(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# Entrenamiento del agente\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple) and len(state) == 2:\n",
    "        state = state[0]  # Si state es un tuple, extraer el estado real\n",
    "    state = discretize_state(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        result = env.step(action)\n",
    "        \n",
    "        # Manejar diferentes retornos de env.step(action)\n",
    "        if len(result) == 4:\n",
    "            next_state, reward, done, _ = result\n",
    "        else:\n",
    "            next_state = result[0]\n",
    "            reward = result[1]\n",
    "            done = result[2]\n",
    "            _ = result[3] if len(result) > 3 else None\n",
    "        \n",
    "        if isinstance(next_state, tuple) and len(next_state) == 2:\n",
    "            next_state = next_state[0]  # Si next_state es un tuple, extraer el estado real\n",
    "        next_state = discretize_state(next_state)\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        td_target = reward + gamma * q_table[next_state][best_next_action]\n",
    "        td_error = td_target - q_table[state][action]\n",
    "        q_table[state][action] += alpha * td_error\n",
    "        state = next_state\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "print(\"Entrenamiento completado.\")\n",
    "print(q_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
