{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'dict' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m     48\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 49\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m, in \u001b[0;36mdiscretize_state\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m     31\u001b[0m     state \u001b[38;5;241m=\u001b[39m [state]\n\u001b[1;32m---> 32\u001b[0m ratios \u001b[38;5;241m=\u001b[39m [(\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_bounds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m/\u001b[39m (state_bounds[i][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m state_bounds[i][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(state))]\n\u001b[0;32m     33\u001b[0m new_state \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m((num_buckets[i] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ratios[i])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(state))]\n\u001b[0;32m     34\u001b[0m new_state \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmin\u001b[39m(num_buckets[i] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, new_state[i])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(state))]\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'dict' and 'float'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "# Crear el entorno\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Discretizar el espacio de observación\n",
    "num_buckets = (1, 1, 6, 12)  # Número de buckets para cada dimensión\n",
    "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "\n",
    "# Ajustar los límites de observación\n",
    "state_bounds[1] = [-0.5, 0.5]\n",
    "state_bounds[3] = [-np.radians(50), np.radians(50)]\n",
    "\n",
    "# Parámetros de Q-Learning\n",
    "alpha = 0.1  # Tasa de aprendizaje\n",
    "gamma = 0.99  # Factor de descuento\n",
    "epsilon = 1.0  # Tasa de exploración\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "num_episodes = 1000\n",
    "\n",
    "# Inicializar la tabla Q\n",
    "q_table = np.zeros(num_buckets + (env.action_space.n,))\n",
    "\n",
    "\n",
    "def discretize_state(state):\n",
    "    if not isinstance(state, (list, tuple)):\n",
    "        state = [state]\n",
    "    ratios = [(state[i] - state_bounds[i][0]) / (state_bounds[i][1] - state_bounds[i][0]) for i in range(len(state))]\n",
    "    new_state = [int(round((num_buckets[i] - 1) * ratios[i])) for i in range(len(state))]\n",
    "    new_state = [min(num_buckets[i] - 1, max(0, new_state[i])) for i in range(len(state))]\n",
    "    return tuple(new_state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def choose_action(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# Entrenamiento del agente\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = discretize_state(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        result = env.step(action)\n",
    "        \n",
    "        # Manejar diferentes retornos de env.step(action)\n",
    "        if len(result) == 4:\n",
    "            next_state, reward, done, _ = result\n",
    "        else:\n",
    "            next_state = result[0]\n",
    "            reward = result[1]\n",
    "            done = result[2]\n",
    "            _ = result[3] if len(result) > 3 else None\n",
    "        \n",
    "        next_state = discretize_state(next_state)\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        td_target = reward + gamma * q_table[next_state][best_next_action]\n",
    "        td_error = td_target - q_table[state][action]\n",
    "        q_table[state][action] += alpha * td_error\n",
    "        state = next_state\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "print(\"Entrenamiento completado.\")\n",
    "\n",
    "# Visualización del agente en acción\n",
    "def run_episode(env, q_table):\n",
    "    frames = []\n",
    "    state = env.reset()\n",
    "    state = discretize_state(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, _, done, _ = env.step(action)\n",
    "        state = discretize_state(state)\n",
    "    env.close()\n",
    "    return frames\n",
    "\n",
    "def save_animation(frames, path='./cartpole.gif', fps=30):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=1000/fps)\n",
    "    anim.save(path, writer='imagemagick', fps=fps)\n",
    "\n",
    "# Generar y guardar la animación\n",
    "frames = run_episode(env, q_table)\n",
    "save_animation(frames)\n",
    "\n",
    "print(\"Animación guardada como cartpole.gif\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
