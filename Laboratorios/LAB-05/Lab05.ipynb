{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LAB-05 REDES NEURONALES - REGRESIÓN LOGÍSTICA MULTICLASE** \n",
    "\n",
    "### Nombre: Gonzales Suyo Franz Reinaldo\n",
    "\n",
    "### Carrera: Ing. de Sistemas\n",
    "\n",
    "### C.U. 35-5335"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del modelo de regresión logística multiclase con una Red Neuronal\n",
    "\n",
    "En este ejercicio implementaremos una red neuronal con regresion logistica multiclase y se aplica a dos diferentes datasets.\n",
    "\n",
    "Nuestro objetuvo es predecir los número de label que son escritos del 0 al 9 en una imagen de 28x28 pixeles.\n",
    "En el siguiente dataset **MNIST FGSM** `minist_train.csv` se encuentran todos los datos.\n",
    "\n",
    "Link del Dataset: https://www.kaggle.com/datasets/sudulakishore/mnist-fgsm?select=mnist_train.csv\n",
    "\n",
    "Link del Repositorio de GitHub LAB-04: https://github.com/Gonzales-Franz-Reinaldo/SIS420-AI/tree/main/Laboratorios/LAB-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Preprocesamiento de los Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Información del Dataset\n",
    "\n",
    "### MNIST FGSM\n",
    "\n",
    "Un conjunto de datos similar al MNIST de 70.000 imágenes de 28 x 28 etiquetadas como Método de señalización de gradiente rápido\n",
    "\n",
    "\n",
    "### Acerca del conjunto de datos\n",
    "\n",
    "#### Contexto\n",
    "\n",
    "MNIST-FGSM es un conjunto de datos de imágenes adversarias que consta de un conjunto de entrenamiento de 60.000 ejemplos y un conjunto de prueba de 10.000 ejemplos. Cada ejemplo es una imagen en escala de grises de 28x28, asociada a una etiqueta de 10 clases. MNIST-FGSM tiene la intención de servir como un reemplazo directo del conjunto de datos original de MNIST para la evaluación comparativa de algoritmos de aprendizaje automático en ejemplos antagónicos. Comparte el mismo tamaño de imagen y la misma estructura de las divisiones de entrenamiento y prueba.\n",
    "\n",
    "El conjunto de datos original de MNIST contiene una gran cantidad de dígitos escritos a mano. A los miembros de la comunidad de IA/ML/Data Science les encanta este conjunto de datos y lo utilizan como punto de referencia para validar sus algoritmos. De hecho, MNIST suele ser el primer conjunto de datos que intentan los investigadores. \"Si no funciona en el MNIST, no funcionará en absoluto\", dijeron. \"Bueno, si funciona en el MNIST, aún puede fallar en otros\".\n",
    "\n",
    "Compruebe la precisión de sus modelos en este conjunto de datos y mejore la solidez adversaria de los modelos\n",
    "\n",
    "#### Contenido\n",
    "\n",
    "Cada imagen tiene 28 píxeles de alto y 28 píxeles de ancho, para un total de 784 píxeles en total. Cada píxel tiene un único valor de píxel asociado, que indica la claridad u oscuridad de ese píxel, y los números más altos significan más oscuro. Este valor de píxel es un número entero entre 0 y 255. Los conjuntos de datos de entrenamiento y prueba tienen 785 columnas. La primera columna consta de las etiquetas de clase (ver arriba) y representa un número entero. El resto de las columnas contienen los valores de píxel de la imagen asociada.\n",
    "\n",
    "Para ubicar un píxel en la imagen, supongamos que hemos descompuesto x como x = i * 28 + j, donde i y j son números enteros entre 0 y 27. El píxel se encuentra en la fila i y la columna j de una matriz de 28 x 28.\n",
    "Por ejemplo, 31 indica el píxel que está en la cuarta columna desde la izquierda y la segunda fila desde la parte superior, como en el diagrama ascii a continuación.\n",
    "\n",
    "#### Descripción\n",
    "\n",
    ". De las columnas: Cada fila es una imagen\n",
    "\n",
    ". independiente, la columna 1 es la etiqueta de la clase (enteros del 0 al 9).\n",
    "\n",
    ". Las columnas restantes son números de píxeles (784 en total).\n",
    "\n",
    ". Cada valor es la oscuridad del píxel (de 1 a 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de las Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# se utiliza para el manejo de rutas y directorios.\n",
    "import os\n",
    "\n",
    "# Calculo cientifico y vectorial para python\n",
    "import numpy as np\n",
    "\n",
    "# Librerias para graficar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Modulo de optimización de scipy\n",
    "from scipy import optimize\n",
    "\n",
    "#Para separa el Dataset 20% y 80% para diferentes pruebas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# para aumentar datos en un dataset\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# le dice a matplotlib que incruste gráficos en el cuaderno\n",
    "%matplotlib inline\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Obtención y Preparación de Datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los datos \n",
    "df_train = pd.read_csv('./mnist_train.csv')\n",
    "df_test = pd.read_csv('./mnist_test.csv')\n",
    "\n",
    "# Configurar Pandas para que no corte la visualización\n",
    "pd.set_option('display.max_rows', 50)  # Mostrar todas las filas (60 -> None)\n",
    "pd.set_option('display.max_columns', None)  # Mostrar todas las columnas (20 -> None)\n",
    "\n",
    "# Mostramos los datos de entrenamiento\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de entrenamiento del 80% \n",
    "X_train = df_train.drop('label', axis=1)\n",
    "y_train = df_train['label']\n",
    "\n",
    "\n",
    "# Datos del prueba del 20 % \n",
    "X_test = df_test.drop('label', axis=1)\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Datos de X_train para el entrenamiento \n",
    "print(\"Datos de X_train\")\n",
    "print(X_train)\n",
    "\n",
    "print('=' * 100)\n",
    "\n",
    "print(\"Datos de y_train\")\n",
    "print(y_train)\n",
    "\n",
    "# Mostramos la cantidad de ejemplos que se utilizaran para el entrenamiento\n",
    "print('=' * 100)\n",
    "print(\"Cantidad de ejemplos del 80% para el entrenamiento es de: {:.0f}\".format(len(X_train)))\n",
    "print(\"Cantidad de ejemplos del 20% para la prueba es de: {:.0f}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos cuantas clases tinene la columna de \"Y\" labels\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los datos de entrenamiento X_train\n",
    "print(\"Datos de entrenamiento\")\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimos todas las clases o labels que contiene la columna de y_train\n",
    "from collections import Counter\n",
    "\n",
    "num_clases = len(np.unique(y_train))\n",
    "print(\"Número de clases:\", num_clases)\n",
    "\n",
    "\n",
    "# Conteo de datos por clase\n",
    "conteo_clases = dict(Counter(y_train))\n",
    "\n",
    "# Imprimir el conteo de datos por clase\n",
    "print(\"Clases podrían ser:\")\n",
    "for clase, conteo in conteo_clases.items():\n",
    "    print(f\"{clase} : {conteo} datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Visualización de los Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def displayData(X, example_width=None, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Muestra datos 2D almacenados en X en una bonita cuadrícula.\n",
    "    \"\"\"\n",
    "    # Compute rows, cols\n",
    "    if X.ndim == 2:\n",
    "        m, n = X.shape\n",
    "    elif X.ndim == 1:\n",
    "        n = X.size\n",
    "        m = 1\n",
    "        X = X[None]  # Promocionar a una matriz bidimensional\n",
    "    else:\n",
    "        raise IndexError('Input X should be 1 or 2 dimensional.')\n",
    "\n",
    "    example_width = example_width or int(np.round(np.sqrt(n)))\n",
    "    example_height = n // example_width  # Cambié esto a división entera\n",
    "\n",
    "    # Compute number of items to display\n",
    "    display_rows = int(np.floor(np.sqrt(m)))\n",
    "    display_cols = int(np.ceil(m / display_rows))\n",
    "\n",
    "    fig, ax_array = plt.subplots(display_rows, display_cols, figsize=figsize)\n",
    "    fig.subplots_adjust(wspace=0.025, hspace=0.025)\n",
    "\n",
    "    ax_array = [ax_array] if m == 1 else ax_array.ravel()\n",
    "\n",
    "    for i, ax in enumerate(ax_array):\n",
    "        # Display Image\n",
    "        h = ax.imshow(X.iloc[i].values.reshape(example_width, example_width),\n",
    "                      cmap='Greys', extent=[0, 1, 0, 1])\n",
    "        ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de ejemplos de entrenamiento\n",
    "m = y_train.size\n",
    "# Se seleccionan 100 datos para ser visualizados\n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "sel = X_train.iloc[rand_indices, :]  # Corregí aquí utilizando iloc\n",
    "displayData(sel)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar algunas imágenes\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(X_train.iloc[i].values.reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Label: {y_train.iloc[i]}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Construcción del Modelo de Red Neuronal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Definición de Capas\n",
    "\n",
    "La red neuronal tiene 3 capas: una capa de entrada, una capa oculta y una capa de salida. Las entradas son valores de píxeles de digitos de imagenes. Dado que las imágenes tienen un tamaño de $28 \\times 28$, esto nos da 784 unidades de capa de entrada (sin contar la unidad de oscilación adicional que siempre genera +1). Los datos de entrenamiento se cargaron en las variables `X` y `y` anteriores.\n",
    "\n",
    "Los parámetros tienen dimensiones que están dimensionadas para una red neuronal con 25 unidades en la segunda capa y 10 unidades de salida (correspondientes a las clases de 10 dígitos ya que van de 0 a 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrucción del modelo de red neuronal\n",
    "# Configuración de parámetros necesario\n",
    "input_layer_size  = 784  # Entrada de imagenes de digitos de 28x28, caracteristicas\n",
    "hidden_layer_size = 25   # 25 hidden units, neuronas ocultas, capa uculta\n",
    "num_labels = 10         # 10 etiquetas, labels del 0 al 9\n",
    "\n",
    "\n",
    "# carga los pesos en las variables Theta1 y Theta2\n",
    "pesos = {}\n",
    "pesos['Theta1'] = np.random.rand(hidden_layer_size, input_layer_size + 1) # 25, 785\n",
    "pesos['Theta2'] = np.random.rand(num_labels, hidden_layer_size + 1)  # 10, 26\n",
    "\n",
    "Theta1 = pesos['Theta1']\n",
    "Theta2 = pesos['Theta2']\n",
    "\n",
    "# Desenrollar parámetros\n",
    "print(Theta1.ravel().shape)  # ravel() para convertir los pesos de una matriz a un vector\n",
    "print(Theta2.ravel().shape)\n",
    "\n",
    "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])\n",
    "print(nn_params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Función de Activación\n",
    "\n",
    "La principal diferencia es que, en el modelo de `regresión logísitca`, utilizaremos una función de activación conocida como `Sigmoid`.\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de activación \n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    g = sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Función de Pérdida (Costo) y Descenso del Gradiente\n",
    "\n",
    "Podríamos intentar entrenar nuestro modelo de `regresión logísitca multiclase` con la función de pérdida que ya conocemos. Sin embargo. Esta función es conocida como *Cross Entropy*.\n",
    "\n",
    "Ahora se implementa la funcion de costo y gradiente para la red neuronal `nnCostFunction`.\n",
    "\n",
    "La función de costo para la red neuronal (con regularización) es:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la función \n",
    "# Esta función calcula la función de costo y los gradientes para una red neuronal de dos capas (una capa oculta) \n",
    "# utilizada en un problema de regresión logística multiclase.\n",
    "\n",
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_):\n",
    "    \n",
    "    # Reformar nn_params nuevamente en los parámetros Theta1 y Theta2, las matrices de peso\n",
    "    # para nuestra red neuronal de 2 capas\n",
    "    \n",
    "    #? Los primeros 150 elementos de nn_params se utilizan para llenar la matriz Theta1, \n",
    "    # que tendrá dimensiones 25 x 785 (25 filas y 785 columnas).\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "    \n",
    "    #? Los elementos apartir de 155 de nn_params se utilizan para llenar la matriz Theta2, \n",
    "    # entonces 260 elementos que tendrá dimensiones 10 x 26 (10 filas y 26 columnas.\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "    \n",
    "    m = y.size\n",
    "    \n",
    "    J = 0\n",
    "    \n",
    "    #? Inicializamos matrices para almacenar los gradientes\n",
    "    Theta2_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    a1 = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "    \n",
    "    a2 = sigmoid(a1.dot(Theta1.T))\n",
    "    a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2], axis=1)\n",
    "    \n",
    "    a3 = sigmoid(a2.dot(Theta2.T))\n",
    "    \n",
    "    y_matrix = y.reshape(-1)\n",
    "    y_matrix = np.eye(num_labels)[y_matrix]\n",
    "    \n",
    "    temp1 = Theta1\n",
    "    temp2 = Theta2\n",
    "    \n",
    "    \n",
    "    #? Calculo de la función de costo con termino de regularización\n",
    "    \n",
    "    regularizacion = (lambda_ / (2 * m)) * (np.sum(np.square(temp1[:, 1:])) + np.sum(np.square(temp2[:, 1:])))\n",
    "    \n",
    "    J = (-1 / m) * np.sum((y_matrix * np.log(a3)) + (1 - y_matrix) * np.log(1 - a3)) + regularizacion\n",
    "    \n",
    "    \n",
    "    #? Backpropagation\n",
    "    \n",
    "    # calcula el error en la capa de salida.\n",
    "    delta_3 = a3 - y_matrix \n",
    "    # calcula el error en la capa oculta propagando hacia atrás el error desde la capa de salida.\n",
    "    delta_2 = delta_3.dot(Theta2)[:, 1:] * sigmoidGradient(a1.dot(Theta1.T))\n",
    "    \n",
    "    # Acumuladores de los gradientes de los pesos Theta1 y Theta2 respectivamente.\n",
    "    Delta1 = delta_2.T.dot(a1)\n",
    "    Delta2 = delta_3.T.dot(a2)\n",
    "    \n",
    "    # Agregar regularización al gradiente\n",
    "    \n",
    "    Theta1_grad = (1 / m) * Delta1\n",
    "    Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (lambda_ / m) * Theta1[:, 1:]\n",
    "    \n",
    "    Theta2_grad = (1 / m) * Delta2\n",
    "    Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (lambda_ / m) * Theta2[:, 1:]\n",
    "    \n",
    "    # Toodas las gradientes\n",
    "    \n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "    \n",
    "    return J, grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebando el funcionamiento de la función de costo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.1\n",
    "\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,  num_labels, X_train, y_train.values, lambda_)\n",
    "print(\"Costo en parámetros (Cargado): %.f \" % J)\n",
    "print('El costo debe esta cercano a          : 0.287629')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializamso la función para inicializar los pesos \n",
    "\n",
    "La función randInitializeWeights se utiliza para inicializar aleatoriamente los pesos de una capa en una red neuronal. Esta inicialización aleatoria es crucial para evitar que todos los pesos comiencen con el mismo valor y se sincronicen durante el entrenamiento, lo que puede llevar a problemas de simetría y convergencia inadecuada durante el aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos los pesos de la red neuronal de forma aleatoria utilizando la función randInitializeWeights\n",
    "def randInitializeWeights(L_in, L_out, epsilon_init = 0.12):\n",
    "    \n",
    "    \"\"\"\n",
    "        Inicializa aleatoriamente los pesos de una capa en una red neuronal.\n",
    "\n",
    "        Parámetros\n",
    "        ----------\n",
    "        L_in:int\n",
    "        Número de conexiones entrantes.\n",
    "\n",
    "        L_salida: int\n",
    "        Número de conexiones salientes.\n",
    "\n",
    "        epsilon_init: flotante, opcional\n",
    "        Rango de valores que puede tomar el peso de un uniforme\n",
    "        distribución.\n",
    "\n",
    "        Devoluciones\n",
    "        -------\n",
    "        W: tipo matriz\n",
    "        El peso inicializado a valores aleatorios. Tenga en cuenta que W debería\n",
    "        establecerse en una matriz de tamaño (L_out, 1 + L_in) como\n",
    "        la primera columna de W maneja los términos de \"sesgo\".\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    . Se crea una matriz W de dimensiones (L_out, 1 + L_in) inicializada con ceros.\n",
    "    . L_out: representa el número de neuronas en la capa actual (conexiones salientes).\n",
    "    . 1 + L_in: se refiere al número de entradas a cada neurona, incluyendo un término de \"sesgo\" (bias) que corresponde a la primera columna de la matriz W.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creamos una matriz con las dimensiones \n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos a la funcion para inicializar los thetas aleatoriamente\n",
    "print(\"Inicialización de parámetros de redes neuronales...\")\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# Desenrrollar parámetos\n",
    "initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Backpropagation\n",
    "\n",
    "Ahora, se implementará el algoritmo de retropropagación. Recuerde que la intuición detrás del algoritmo de retropropagación es la siguiente. Dado un ejemplo de entrenamiento $(x^{(t)}, y^{(t)})$, primero ejecutaremos un \"pase hacia adelante\" para calcular todas las activaciones en toda la red, incluido el valor de salida de la hipótesis $h_\\theta(x)$. Luego, para cada nodo $j$ en la capa $l$, se busca calcular un \"término de error\" $\\delta_j^{(l)}$ que mide cuánto ese nodo fue \"responsable\" de cualquier error en la salida.\n",
    "\n",
    "### 3.4  Comprobación del gradiente\n",
    "\n",
    "En la red neuronal, se está minimizando la función de costo $J(\\Theta)$. Para realizar una verificación de gradiente en sus parámetros, se puede imaginar \"desenrollar\" los parámetros $\\Theta^{(1)}$, $\\Theta^{(2)}$ en un vector largo $\\theta$. Al hacerlo, se puede pensar que la función de costo es $J(\\Theta)$ y usar el siguiente procedimiento de verificación de gradiente.\n",
    "\n",
    "\n",
    "** Consejo práctico **: al realizar la verificación de gradientes, es mucho más eficiente utilizar una pequeña red neuronal con un número relativamente pequeño de unidades de entrada y unidades ocultas, por lo que tiene un número relativamente pequeño\n",
    "de parámetros. Cada dimensión de $\\theta$ requiere dos evaluaciones de la función de costo y esto puede resultar costoso. En la función `checkNNGradients`, nuestro código crea un pequeño modelo aleatorio y un conjunto de datos que se usa con `computeNumericalGradient` para verificar el gradiente. Además, una vez que esté seguro de que sus cálculos de gradiente son correctos, debe desactivar la verificación de gradiente antes de ejecutar su algoritmo de aprendizaje.\n",
    "\n",
    "\n",
    "** Sugerencia práctica: ** La verificación del gradiente funciona para cualquier función en la que esté calculando el costo y el gradiente. Concretamente, puede usar la misma función `computeNumericalGradient` para verificar si sus implementaciones de gradiente para los otros ejercicios también son correctas (por ejemplo, la función de costo de regresión logística)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? Function para depurar Inicializar pesos\n",
    "def debugInitializeWeights(fan_out, fan_in):\n",
    "    \"\"\"\n",
    "    Inicializar los pesos de una capa con conexiones entrantes fan_in y salidas fan_out\n",
    "    conexiones usando una estrategia fija. Esto le ayudará más adelante en la depuración.\n",
    "\n",
    "    Tenga en cuenta que W debe establecerse como una matriz de tamaño (1+fan_in, fan_out) como la primera fila de W maneja\n",
    "    los términos de \"sesgo\".\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    fan_out: int\n",
    "    El número de conexiones salientes.\n",
    "\n",
    "    fan_in: int\n",
    "    El número de conexiones entrantes.\n",
    "\n",
    "    Devoluciones\n",
    "    -------\n",
    "    W: array_like (1+fan_in, fan_out)\n",
    "    La matriz de pesos inicializada dadas las dimensiones.\n",
    "    \"\"\"\n",
    "    # Initialize W using \"sin\". This ensures that W is always of the same values and will be\n",
    "    # useful for debugging\n",
    "    W = np.sin(np.arange(1, 1 + (1+fan_in)*fan_out))/10.0\n",
    "    W = W.reshape(fan_out, 1+fan_in, order='F')\n",
    "    \n",
    "    return W\n",
    "\n",
    "\n",
    "#? Función para calcular gradiente numérico\n",
    "def computeNumericalGradient(J, theta, e=1e-4):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente usando \"diferencias finitas\" y nos da una estimación numérica del\n",
    "    degradado.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    J: función\n",
    "    La función de costo que se utilizará para estimar su gradiente numérico.\n",
    "\n",
    "    theta: tipo matriz\n",
    "    Los parámetros de red unidimensionales desenrollados. El gradiente numérico se calcula en\n",
    "    esos parámetros dados.\n",
    "\n",
    "    e: flotante (opcional)\n",
    "    El valor que se utilizará para épsilon para calcular la diferencia finita.\n",
    "\n",
    "    Notas\n",
    "    -----\n",
    "    El siguiente código implementa la verificación de gradiente numérico y\n",
    "    devuelve el gradiente numérico. Establece `numgrad[i]` en (un valor numérico\n",
    "    aproximación de) la derivada parcial de J con respecto a la\n",
    "    i-ésimo argumento de entrada, evaluado en theta. (es decir, `numgrad[i]` debería\n",
    "    ser (aproximadamente) la derivada parcial de J con respecto\n",
    "    a theta[i].)\n",
    "    \"\"\"\n",
    "    numgrad = np.zeros(theta.shape)\n",
    "    perturb = np.diag(e * np.ones(theta.shape))\n",
    "    for i in range(theta.size):\n",
    "        loss1, _ = J(theta - perturb[:, i])\n",
    "        loss2, _ = J(theta + perturb[:, i])\n",
    "        numgrad[i] = (loss2 - loss1)/(2*e)\n",
    "        \n",
    "    return numgrad\n",
    "\n",
    "\n",
    "#? Función para comprobar gradientes NN\n",
    "def checkNNGradients(nnCostFunction, lambda_=0):\n",
    "    \"\"\"\n",
    "    Crea una pequeña red neuronal para comprobar los gradientes de retropropagación. Dará salida al\n",
    "    gradientes analíticos producidos por su código backprop y los gradientes numéricos\n",
    "    (calculado usando ComputeNumericalGradient). Estos dos cálculos de gradiente deberían dar como resultado\n",
    "    valores muy similares.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    nnCostoFunción: func\n",
    "    Una referencia a la función de costos implementada por el estudiante.\n",
    "\n",
    "    lambda_: flotante (opcional)\n",
    "    El valor del parámetro de regularización.\n",
    "    \"\"\"\n",
    "    input_layer_size = 784\n",
    "    hidden_layer_size = 25\n",
    "    num_labels = 10\n",
    "    m = 25\n",
    "\n",
    "    # We generate some 'random' test data\n",
    "    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)\n",
    "    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)\n",
    "\n",
    "    # Reusing debugInitializeWeights to generate X\n",
    "    X = debugInitializeWeights(m, input_layer_size - 1)\n",
    "    y = np.arange(1, 1+m) % num_labels\n",
    "    # print(y)\n",
    "    # Unroll parameters\n",
    "    nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])\n",
    "\n",
    "    # short hand for cost function\n",
    "    costFunc = lambda p: nnCostFunction(p, input_layer_size, hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "    cost, grad = costFunc(nn_params)\n",
    "    numgrad = computeNumericalGradient(costFunc, nn_params)\n",
    "\n",
    "    # Visually examine the two gradient computations.The two columns you get should be very similar.\n",
    "    print(np.stack([numgrad, grad], axis=1))\n",
    "    print('Las dos columnas anteriores que obtenga deberían ser muy similares.')\n",
    "    print('(Izquierda: su gradiente numérico, gradiente analítico derecho)\\n')\n",
    "\n",
    "    # Evaluate the norm of the difference between two the solutions. If you have a correct\n",
    "    # implementation, and assuming you used e = 0.0001 in computeNumericalGradient, then diff\n",
    "    # should be less than 1e-9.\n",
    "    diff = np.linalg.norm(numgrad - grad)/np.linalg.norm(numgrad + grad)\n",
    "\n",
    "    print('Si su implementación de retropropagación es correcta, entonces \\n'\n",
    "          'la diferencia relativa será pequeña (menos de 1e-9).. \\n'\n",
    "          'Diferencia relativa: %g' % diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos la prueba de la función \n",
    "checkNNGradients(nnCostFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo de forma regularizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos los gradientes ejecutando checkNNGradients\n",
    "lambda_ = 3\n",
    "checkNNGradients(nnCostFunction, lambda_)\n",
    "\n",
    "# También genera los valores de depuración de costFunction\n",
    "debug_J, _  = nnCostFunction(nn_params, input_layer_size,\n",
    "                          hidden_layer_size, num_labels, X_train, y_train.values, lambda_)\n",
    "\n",
    "print('\\n\\nCosto en parámetros de depuración (fijos) (w/ lambda = %f): %f ' % (lambda_, debug_J))\n",
    "print('(for lambda = 3, this value should be about 0.576051)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Entrenamiento del Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo \n",
    "\n",
    "# Después de haber completado la tarea, cambie el maxiter a uno más grande\n",
    "#? valor para ver cómo ayuda más formación.\n",
    "# Especifica el número máximo de iteraciones que el algoritmo de optimización \n",
    "options = {'maxiter': 100}\n",
    "\n",
    "#? Probar con diferentes valores de lambda\n",
    "lambda_ = 1\n",
    "\n",
    "#? Cree una \"taquigrafía\" para minimizar la función de costos.\n",
    "#? Ahora, costFunction es una función que toma solo un argumento.\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size, \n",
    "                                        hidden_layer_size, \n",
    "                                        num_labels, X_train, y_train, lambda_)\n",
    "\n",
    "#? Ahora, costFunction es una función que toma solo un argumento.\n",
    "#? (los parámetros de la red neuronal)\n",
    "\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "#? obtenemos la solución de la optimización\n",
    "nn_params = res.x\n",
    "\n",
    "#? Obtenemos Theta1 y Theta2 de nn_params\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
